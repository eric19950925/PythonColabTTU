{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PythonTTU.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPXKzw4E3E164xqTOSpC+LE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eric19950925/PythonColabTTU/blob/main/PythonTTU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjxmSACKHdqU"
      },
      "source": [
        "練習1:直接爬原始碼 HTML response\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tX55XcvazBjI"
      },
      "source": [
        "from urllib import request\n",
        "url = \"https://ctee.com.tw/livenews/aj\"\n",
        "src_url = \"https://github.com/eric19950925/PythonColabTTU/blob/main/PythonTTU.ipynb\"\n",
        "res = request.urlopen(url)\n",
        "content = res.read().decode('utf-8')\n",
        "print(content)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wD0DuCxH-vJ"
      },
      "source": [
        "練習2:爬指定字串 Scraping and Parsing the data with parser"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joqBN4LMU5no"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests as rq\n",
        "r = rq.get(url)\n",
        "soup = BeautifulSoup(r.text,\"lxml\")\n",
        "print(r)\n",
        "print(soup.find('h1').text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xd0GXKx6NXAj"
      },
      "source": [
        "例外處理 Exception Handling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-Y9kAmRNZK5"
      },
      "source": [
        "def get_tag_text(url,tag):\n",
        "  try:\n",
        "    resp = rq.get(url)\n",
        "    if resp.status_code == 200:\n",
        "      soup = BeautifulSoup(r.text,\"lxml\")\n",
        "      return soup.find(tag).text\n",
        "  except Exception as e:\n",
        "    print('Exception:%s'%(e))\n",
        "  return None\n",
        "\n",
        "bad_url = \"\"\n",
        "print(get_tag_text(url,'h1'))\n",
        "print(get_tag_text(url,'h88'))\n",
        "print(get_tag_text(bad_url,'h1'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFdXvfn4KqJU"
      },
      "source": [
        "練習3:新聞文章彙整\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsC030hQzTbr"
      },
      "source": [
        "def get_web_page(url):\n",
        "  resp = rq.get(\n",
        "      url = url,\n",
        "      cookies = {'over18': '1'}\n",
        "  )\n",
        "  if resp.status_code != 200:\n",
        "    print('invalid url:',resp.url)\n",
        "    return None\n",
        "  else: \n",
        "    return resp.text"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwpRfJpy53l8"
      },
      "source": [
        "def get_articles(dom,date):\n",
        "  soup = BeautifulSoup(dom,'html5lib')\n",
        "\n",
        "  paging_div = soup.find('div','btn-group btn-group-paging')\n",
        "  prev_url = paging_div.find_all('a')[1]['href']\n",
        "\n",
        "  articles = []\n",
        "  divs = soup.find_all('div','r-ent')\n",
        "\n",
        "  for d in divs:\n",
        "    if d.find('div','date').text.strip() == date:\n",
        "      push_count = 0\n",
        "      push_str = d.find('div','nrec').text\n",
        "\n",
        "      if push_str:\n",
        "        try:\n",
        "          push_count = int(push_str)\n",
        "        except ValueError:\n",
        "          if push_str == '爆':\n",
        "              push_count == 99\n",
        "          elif push_str.startswith('X'):\n",
        "            push_count == -10\n",
        "\n",
        "      if d.find('a'):\n",
        "        href = d.find('a')['href']\n",
        "        title = d.find('a').text\n",
        "        author = d.find('div','author').text if d.find('div','author') else ''\n",
        "        articles.append({\n",
        "            'title':title,\n",
        "            'href':href,\n",
        "            'push_count':push_count,\n",
        "            'author':author\n",
        "            })\n",
        "\n",
        "  return articles,prev_url"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceOIBNRb55uX"
      },
      "source": [
        "PTT_URL = 'https://www.ptt.cc'\n",
        "import time\n",
        "import json\n",
        "\n",
        "current_page=get_web_page(PTT_URL + '/bbs/Gossiping/index.html')\n",
        "\n",
        "if current_page:\n",
        "  articles = []\n",
        "\n",
        "  today = time.strftime(\"%m/%d\").lstrip('0')\n",
        "\n",
        "  current_articles,prev_url = get_articles(current_page,today)\n",
        "\n",
        "  while current_articles:\n",
        "    articles += current_articles\n",
        "    current_page = get_web_page(PTT_URL + prev_url)\n",
        "    current_articles,prev_url = get_articles(current_page,today)\n",
        "\n",
        "  print('今天有',len(articles),'篇文章')\n",
        "  threshold = 50\n",
        "  print('熱門文章(>%d推):'%(threshold))\n",
        "  for a in articles:\n",
        "    if int(a['push_count'])>threshold:\n",
        "      print(a)\n",
        "  \n",
        "  with open('gossiping.json','w',encoding='utf-8') as f:\n",
        "    json.dump(articles, f, indent=2, sort_keys=True, ensure_ascii=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}